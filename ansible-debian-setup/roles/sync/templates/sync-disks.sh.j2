#!/bin/bash

# Настройки
REMOTE_IP="192.168.0.104"
REMOTE_USER="agger"
REMOTE_PASS="2864"
MOUNT_PREFIX="/mnt"
BACKEND_CONFIG="/home/apper/iqbanana-disk/backend/config/config.js"
LOG_FILE="/var/log/sync-disks.log"
TRIGGER_FILE="/tmp/sync_trigger"

# Функция для логирования
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Создаём лог-файл с правильными правами, если его нет
mkdir -p /var/log
touch "$LOG_FILE"
sudo chown apper:apper "$LOG_FILE"
sudo chmod 644 "$LOG_FILE"

# Проверка триггера или принудительного запуска
if [ ! -f "$TRIGGER_FILE" ] && [ "$1" != "--force" ]; then
    log "Триггер $TRIGGER_FILE не найден, пропускаю выполнение"
    exit 0
fi

# Удаляем триггер после активации
[ -f "$TRIGGER_FILE" ] && sudo rm -f "$TRIGGER_FILE" 2>/dev/null

# Установка необходимых пакетов
if ! command -v cifs-utils &> /dev/null; then
    log "Устанавливаем cifs-utils..."
    sudo apt-get update && sudo apt-get install -y cifs-utils
fi

# Проверка связи с сервером agger
log "Проверка связи с $REMOTE_IP..."
if ! ping -c 2 -W 5 $REMOTE_IP > /dev/null 2>&1; then
    log "ОШИБКА: Сервер $REMOTE_IP недоступен"
    exit 1
fi

# Проверяем доступность Samba на сервере
log "Проверка доступности порта Samba на $REMOTE_IP..."
if ! nc -z -w2 $REMOTE_IP 445; then
    log "ОШИБКА: Порт Samba (445) недоступен на сервере $REMOTE_IP"
    exit 1
fi

# Сканируем диски на сервере agger
log "Сканирование дисков на сервере $REMOTE_IP..."
DISKS=$(sshpass -p "$REMOTE_PASS" ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10 $REMOTE_USER@$REMOTE_IP "find /mnt -maxdepth 1 -name 'disk_*' -type d | sort" 2>/dev/null)

if [ -z "$DISKS" ]; then
    log "ОШИБКА: Не удалось найти диски на сервере $REMOTE_IP"
    exit 1
fi

# Логируем список найденных дисков
log "Найденные диски на сервере $REMOTE_IP:"
echo "$DISKS" | while IFS= read -r line; do log "  $line"; done

# Создаем директории для конфигурации приложения
CONFIG_DISKS=" disks: {"

# Очищаем точки монтирования
log "Размонтирование старых точек монтирования..."
for mount in $(mount | grep "$MOUNT_PREFIX/disk_" | awk '{print $3}'); do
    sudo umount -f -l "$mount" 2>/dev/null || log "Ошибка размонтирования $mount"
    sleep 1
done

# Обработка найденных дисков
while IFS= read -r remote_disk; do
    if [ -z "$remote_disk" ]; then
        continue
    fi

    disk_name=$(basename "$remote_disk")
    local_mount="$MOUNT_PREFIX/$disk_name"
    
    # Преобразуем имя для CIFS шары (например, disk_sda1 -> DISK_SDA1)
    share_name="DISK_$(echo ${disk_name#disk_} | tr '[:lower:]' '[:upper:]')"
    
    log "Обработка диска $disk_name, шара $share_name, монтирование в $local_mount"

    # Создаем локальную точку монтирования
    if [ ! -d "$local_mount" ]; then
        log "Создание точки монтирования $local_mount"
        sudo mkdir -p "$local_mount"
        sudo chown apper:apper "$local_mount"
        sudo chmod 775 "$local_mount"
    fi

    # Монтируем через CIFS
    if ! mount | grep -q "$local_mount"; then
        if ! sudo mount -t cifs "//$REMOTE_IP/$share_name" "$local_mount" -o username=$REMOTE_USER,password=$REMOTE_PASS,vers=3.0,uid=$(id -u apper),gid=$(id -g apper),file_mode=0777,dir_mode=0777,iocharset=utf8,cache=strict; then
            log "ПРЕДУПРЕЖДЕНИЕ: Не удалось смонтировать шару $share_name, пробуем прямое соединение через SSH..."
            
            # Альтернативное решение: монтирование через SSHFS
            sudo mkdir -p "$local_mount"
            sudo chown apper:apper "$local_mount"
            
            if sshfs -o StrictHostKeyChecking=no,password_stdin,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3,allow_other,default_permissions,idmap=user,uid=$(id -u apper),gid=$(id -g apper) $REMOTE_USER@$REMOTE_IP:$remote_disk $local_mount <<< "$REMOTE_PASS"; then
                log "Успешно смонтирован $remote_disk через SSHFS"
                CONFIG_DISKS+="\n '$disk_name': '$local_mount',"
            else
                log "ОШИБКА: Не удалось смонтировать $remote_disk ни через CIFS, ни через SSHFS"
            fi
        else
            log "Успешно смонтирован //$REMOTE_IP/$share_name через CIFS"
            CONFIG_DISKS+="\n '$disk_name': '$local_mount',"
        fi
    else
        log "Точка $local_mount уже смонтирована"
        CONFIG_DISKS+="\n '$disk_name': '$local_mount',"
    fi
done <<< "$DISKS"

# Завершаем конфигурацию приложения
CONFIG_DISKS=$(echo -e "$CONFIG_DISKS" | sed '$s/,$//')
CONFIG_DISKS+="\n }"

# Обновляем конфигурацию веб-приложения
log "Обновление конфигурации веб-приложения..."
if [ -f "$BACKEND_CONFIG" ]; then
    # Создаем резервную копию
    cp "$BACKEND_CONFIG" "$BACKEND_CONFIG.backup.$(date +%Y%m%d%H%M%S)"

    # Обновляем секцию disks в конфигурации
    awk -v disks="$CONFIG_DISKS" '
    /disks:/ {
        print disks
        in_disks = 1
        next
    }
    in_disks && /}/ {
        in_disks = 0
        next
    }
    in_disks {
        next
    }
    { print }
    ' "$BACKEND_CONFIG" > /tmp/config.js.new

    sudo mv /tmp/config.js.new "$BACKEND_CONFIG"
    sudo chown apper:apper "$BACKEND_CONFIG" 
    log "Конфигурация веб-приложения обновлена"

    # Перезапускаем backend-сервис, если он активен
    if systemctl is-active --quiet backend; then
        log "Перезапуск backend-сервиса..."
        sudo systemctl restart backend
    else
        log "Backend-сервис не активен, пропускаю перезапуск"
    fi
else
    log "ОШИБКА: Файл конфигурации $BACKEND_CONFIG не найден"
fi

# Проверяем, смонтированы ли диски
log "Проверка смонтированных дисков..."
mount_count=$(mount | grep "on $MOUNT_PREFIX/disk_" | wc -l)
log "Успешно смонтировано $mount_count дисков"

# Итоги
log "Синхронизация дисков завершена"
exit 0