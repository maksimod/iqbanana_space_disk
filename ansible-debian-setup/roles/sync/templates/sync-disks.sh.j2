#!/bin/bash

# Настройки
REMOTE_IP="192.168.0.104"
REMOTE_USER="agger"
REMOTE_PASS="2864"  # Пароль пользователя agger на сервере 192.168.0.104
MOUNT_PREFIX="/mnt"
BACKEND_CONFIG="/home/apper/iqbanana-disk/backend/config/config.js"
LOG_FILE="/var/log/sync-disks.log"
TRIGGER_FILE="/tmp/sync_trigger"

# Установка sshpass, если не установлен
if ! command -v sshpass &> /dev/null; then
    echo "Устанавливаем sshpass..."
    sudo apt-get update && sudo apt-get install -y sshpass
fi

# Функция для логирования
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Создаём лог-файл с правильными правами, если его нет
mkdir -p /var/log
touch "$LOG_FILE"
sudo chown apper:apper "$LOG_FILE"
sudo chmod 644 "$LOG_FILE"

# Проверка триггера или принудительного запуска
if [ ! -f "$TRIGGER_FILE" ] && [ "$1" != "--force" ]; then
    log "Триггер $TRIGGER_FILE не найден, пропускаю выполнение"
    exit 0
fi

# Удаляем триггер после активации
[ -f "$TRIGGER_FILE" ] && sudo rm -f "$TRIGGER_FILE" 2>/dev/null

# Очистка старых ключей хостов
if [ -f "/home/apper/.ssh/known_hosts" ]; then
    log "Очистка известных ключей хостов для 192.168.0.104..."
    ssh-keygen -f "/home/apper/.ssh/known_hosts" -R "192.168.0.104" 2>/dev/null || true
fi

# Проверка SSH-соединения с использованием пароля
log "Проверка SSH-доступа к $REMOTE_IP..."
if ! sshpass -p "$REMOTE_PASS" ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10 $REMOTE_USER@$REMOTE_IP "echo test" >/dev/null 2>&1; then
    log "ОШИБКА: Не удалось подключиться к $REMOTE_IP по SSH"
    exit 1
fi

# Получение списка доступных дисков с Agger сервера
log "Получение списка доступных дисков с Agger сервера..."
REMOTE_DISKS=$(sshpass -p "$REMOTE_PASS" ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null $REMOTE_USER@$REMOTE_IP "find /mnt -maxdepth 1 -name 'disk_*' -type d | sort")

if [ -z "$REMOTE_DISKS" ]; then
    log "ОШИБКА: Не удалось получить список дисков с Agger сервера"
    exit 1
fi

# Логируем список доступных дисков
log "Доступные диски на Agger сервере:"
echo "$REMOTE_DISKS" | while IFS= read -r line; do log "  $line"; done

# Проверить и установить sshfs, если необходимо
if ! command -v sshfs &> /dev/null; then
    log "Устанавливаем sshfs..."
    sudo apt-get update && sudo apt-get install -y sshfs
fi

# Создаем директории для конфигурации приложения
CONFIG_DISKS=" disks: {"

# Очищаем точки монтирования
log "Размонтирование старых точек монтирования..."
for mount in $(mount | grep "$MOUNT_PREFIX/disk_" | awk '{print $3}'); do
    sudo umount -f -l "$mount" 2>/dev/null || log "Ошибка принудительного размонтирования $mount"
    sleep 1
done

# Обрабатываем каждый диск с удаленного сервера
while IFS= read -r remote_disk; do
    disk_name=$(basename "$remote_disk")
    local_mount="$MOUNT_PREFIX/$disk_name"
    
    # Создаем локальную точку монтирования
    if [ ! -d "$local_mount" ]; then
        log "Создание точки монтирования $local_mount"
        sudo mkdir -p "$local_mount"
        sudo chown apper:apper "$local_mount"
        sudo chmod 775 "$local_mount"
    fi
    
    # Монтируем через sshfs с использованием пароля
    if ! mount | grep -q "$local_mount"; then
        echo "$REMOTE_PASS" | sudo sshfs -o password_stdin,StrictHostKeyChecking=no,UserKnownHostsFile=/dev/null,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3,allow_other,default_permissions,idmap=user,uid=$(id -u apper),gid=$(id -g apper) $REMOTE_USER@$REMOTE_IP:$remote_disk $local_mount
        if [ $? -eq 0 ]; then
            log "Успешно смонтирован $remote_disk на $local_mount через sshfs"
            # Добавляем в конфигурацию приложения
            CONFIG_DISKS+="\n '$disk_name': '$local_mount',"
        else
            log "ОШИБКА: Не удалось смонтировать $remote_disk через sshfs"
        fi
    else
        log "Точка $local_mount уже смонтирована"
        # Добавляем в конфигурацию приложения
        CONFIG_DISKS+="\n '$disk_name': '$local_mount',"
    fi
done <<< "$REMOTE_DISKS"

# Завершаем конфигурацию приложения
CONFIG_DISKS=$(echo -e "$CONFIG_DISKS" | sed '$s/,$//')
CONFIG_DISKS+="\n }"

# Обновляем конфигурацию веб-приложения
log "Обновление конфигурации веб-приложения..."
if [ -f "$BACKEND_CONFIG" ]; then
    # Создаем резервную копию
    cp "$BACKEND_CONFIG" "$BACKEND_CONFIG.backup.$(date +%Y%m%d%H%M%S)"

    # Обновляем секцию disks в конфигурации
    awk -v disks="$CONFIG_DISKS" '
    /disks:/ {
        print disks
        in_disks = 1
        next
    }
    in_disks && /}/ {
        in_disks = 0
        next
    }
    in_disks {
        next
    }
    { print }
    ' "$BACKEND_CONFIG" > /tmp/config.js.new

    sudo mv /tmp/config.js.new "$BACKEND_CONFIG"
    sudo chown apper:apper "$BACKEND_CONFIG" 
    log "Конфигурация веб-приложения обновлена"

    # Перезапускаем backend-сервис, если он активен
    if systemctl is-active --quiet backend; then
        log "Перезапуск backend-сервиса..."
        sudo systemctl restart backend
    else
        log "Backend-сервис не активен, пропускаю перезапуск"
    fi
else
    log "ОШИБКА: Файл конфигурации $BACKEND_CONFIG не найден"
fi

# Проверяем, смонтированы ли диски
log "Проверка смонтированных дисков..."
mount_count=$(mount | grep sshfs | wc -l)
log "Успешно смонтировано $mount_count дисков"

# Проверка стабильности соединения
log "Проверка стабильности сетевого соединения..."
if ping -c 5 -w 10 $REMOTE_IP >/dev/null 2>&1; then
    log "Сетевое соединение с $REMOTE_IP стабильно"
else
    log "ПРЕДУПРЕЖДЕНИЕ: Возможны проблемы с сетевым соединением"
fi

# Итоги
log "Синхронизация дисков завершена"
exit 0